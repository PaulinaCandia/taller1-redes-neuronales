{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits  # Usaremos un dataset de ejemplo\n",
    "\n",
    "# Definimos las funciones de activación y pérdida\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "# Clase para las capas densas\n",
    "class FCLayer:\n",
    "    def __init__(self, input_size, output_size, lambda_reg=0.0):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient)\n",
    "\n",
    "        # Regularización L2\n",
    "        weights_gradient += self.lambda_reg * self.weights\n",
    "\n",
    "        # Actualización de pesos y bias\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias -= learning_rate * np.mean(output_gradient, axis=0)\n",
    "        return input_gradient\n",
    "\n",
    "# Clase para las capas de activación\n",
    "class ActivationLayer:\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return output_gradient * self.activation_prime(self.input)\n",
    "\n",
    "# Clase para la red neuronal\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        for i in range(samples):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Entrenamiento con mini batch\n",
    "    def fit(self, X_train, y_train, epochs, learning_rate, batch_size):\n",
    "        samples = len(X_train)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch+1}/{epochs}')\n",
    "            # Mezclar los datos de entrenamiento en cada época\n",
    "            indices = np.arange(samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            # Procesar por mini batches\n",
    "            for i in range(0, samples, batch_size):\n",
    "                X_batch = X_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                output = X_batch\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward(output)\n",
    "\n",
    "                # Calcular la pérdida\n",
    "                loss = self.loss(y_batch, output)\n",
    "\n",
    "                # Backward pass\n",
    "                output_gradient = self.loss_prime(y_batch, output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    output_gradient = layer.backward(output_gradient, learning_rate)\n",
    "\n",
    "            print(f'Loss: {loss}')\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# One-hot encoding para las etiquetas de salida\n",
    "y_one_hot = np.zeros((y.size, 10))\n",
    "y_one_hot[np.arange(y.size), y] = 1\n",
    "\n",
    "# Dividir el dataset en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear la red neuronal\n",
    "entrada_dim = X_train.shape[1]\n",
    "model = Network()\n",
    "\n",
    "# Definir regularización y capas\n",
    "lambda_reg = 0.001\n",
    "model.add(FCLayer(entrada_dim, 256, lambda_reg=lambda_reg))  # Capa con regularización L2\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(256, 256, lambda_reg=lambda_reg))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(256, 10, lambda_reg=lambda_reg))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar la función de pérdida\n",
    "model.use(mse, mse_prime)\n",
    "\n",
    "# Entrenar la red usando mini batches\n",
    "batch_size = 32  # Hiperparámetro: tamaño del mini batch\n",
    "model.fit(X_train, y_train, epochs=20, learning_rate=0.1, batch_size=batch_size)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformar las predicciones a etiquetas\n",
    "y_hat = [np.argmax(i) for i in y_hat]\n",
    "y_test_labels = [np.argmax(i) for i in y_test]\n",
    "\n",
    "# Evaluar el modelo\n",
    "print('Matriz de confusión:')\n",
    "print(confusion_matrix(y_test_labels, y_hat))\n",
    "print(f'Exactitud del modelo: {accuracy_score(y_test_labels, y_hat)}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
