{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Gdo-kKeWrAf"
      },
      "outputs": [],
      "source": [
        "# Base\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / y_true.size\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementación de la capa densa (FCLayer)\n",
        "class FCLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
        "        weights_gradient = np.dot(self.input.T, output_gradient)\n",
        "\n",
        "\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.bias -= learning_rate * np.mean(output_gradient, axis=0)\n",
        "        return input_gradient\n"
      ],
      "metadata": {
        "id": "si1f0wXVWvS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementación de la capa de activación\n",
        "class ActivationLayer:\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.output = self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return output_gradient * self.activation_prime(self.input)\n"
      ],
      "metadata": {
        "id": "P0CsVRAnWy7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementación de la capa Dropout\n",
        "class DropoutLayer:\n",
        "    def __init__(self, rate=0.5):\n",
        "        self.rate = rate\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.mask = np.random.binomial(1, 1 - self.rate, size=input.shape)\n",
        "        return input * self.mask\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return output_gradient * self.mask\n"
      ],
      "metadata": {
        "id": "74m6ShpPW0w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementación de la red neuronal con Early Stopping\n",
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_prime = None\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def use(self, loss, loss_prime):\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        samples = len(input_data)\n",
        "        result = []\n",
        "\n",
        "        for i in range(samples):\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward(output)\n",
        "            result.append(output)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def fit(self, X_train, y_train, epochs, learning_rate, batch_size, X_val=None, y_val=None, patience=5):\n",
        "        samples = len(X_train)\n",
        "        best_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f'Epoch {epoch+1}/{epochs}')\n",
        "            indices = np.arange(samples)\n",
        "            np.random.shuffle(indices)\n",
        "            X_train = X_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "\n",
        "            for i in range(0, samples, batch_size):\n",
        "                X_batch = X_train[i:i + batch_size]\n",
        "                y_batch = y_train[i:i + batch_size]\n",
        "\n",
        "                output = X_batch\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward(output)\n",
        "\n",
        "                loss = self.loss(y_batch, output)\n",
        "                output_gradient = self.loss_prime(y_batch, output)\n",
        "\n",
        "                for layer in reversed(self.layers):\n",
        "                    output_gradient = layer.backward(output_gradient, learning_rate)\n",
        "\n",
        "            # VEarly Stopping\n",
        "            if X_val is not None and y_val is not None:\n",
        "                val_output = self.predict(X_val)\n",
        "                val_loss = self.loss(y_val, val_output)\n",
        "\n",
        "                if val_loss < best_loss:\n",
        "                    best_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "                if patience_counter >= patience:\n",
        "                    print(\"Early stopping triggered.\")\n",
        "                    return\n",
        "\n",
        "            print(f'Loss: {loss}')\n"
      ],
      "metadata": {
        "id": "9PHLOrDFW2hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Uso del modelo con Dropout y Early Stopping\n",
        "\n",
        "# Crear red neuronal\n",
        "network = Network()\n",
        "network.add(FCLayer(2, 3))  # Capa densa\n",
        "network.add(ActivationLayer(sigmoid, sigmoid_prime))  # Capa de activación\n",
        "network.add(DropoutLayer(0.2))  # Dropout con un rate del 20%\n",
        "network.add(FCLayer(3, 1))  # Capa de salida\n",
        "\n",
        "network.use(mse, mse_prime)\n",
        "\n",
        "# Datos de ejemplo\n",
        "X_train = np.random.rand(100, 2)\n",
        "y_train = np.random.rand(100, 1)\n",
        "\n",
        "X_val = np.random.rand(20, 2)\n",
        "y_val = np.random.rand(20, 1)\n",
        "\n",
        "\n",
        "network.fit(X_train, y_train, epochs=100, learning_rate=0.1, batch_size=32, X_val=X_val, y_val=y_val, patience=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdX6NgyIW46D",
        "outputId": "785f4454-3a55-4159-8300-4f8d1c8c433e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Loss: 0.15528773641449375\n",
            "Epoch 2/100\n",
            "Loss: 0.02912862288533439\n",
            "Epoch 3/100\n",
            "Loss: 0.03402584629396291\n",
            "Epoch 4/100\n",
            "Loss: 0.19151787720542057\n",
            "Epoch 5/100\n",
            "Loss: 0.10967333544584781\n",
            "Epoch 6/100\n",
            "Loss: 0.11557853704343969\n",
            "Epoch 7/100\n",
            "Loss: 0.10184223041129317\n",
            "Epoch 8/100\n",
            "Loss: 0.058497246014361876\n",
            "Epoch 9/100\n",
            "Loss: 0.0883089760291196\n",
            "Epoch 10/100\n",
            "Loss: 0.06736839239748905\n",
            "Epoch 11/100\n",
            "Loss: 0.10677688590636983\n",
            "Epoch 12/100\n",
            "Loss: 0.1384568452461558\n",
            "Epoch 13/100\n",
            "Loss: 0.16139330490274217\n",
            "Epoch 14/100\n",
            "Loss: 0.14107077920825595\n",
            "Epoch 15/100\n",
            "Loss: 0.08091716559258565\n",
            "Epoch 16/100\n",
            "Loss: 0.1532825450887873\n",
            "Epoch 17/100\n",
            "Loss: 0.08053105239723576\n",
            "Epoch 18/100\n",
            "Loss: 0.030082400274243835\n",
            "Epoch 19/100\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    }
  ]
}